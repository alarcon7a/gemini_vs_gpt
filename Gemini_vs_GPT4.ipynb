{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alarcon7a/gemini_vs_gpt/blob/main/Gemini_vs_GPT4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "BpP_YnWlCKq-"
      },
      "source": [
        "## Instalando algunas librerias necesarias"
      ],
      "id": "BpP_YnWlCKq-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d8859cb5d4d05fa",
        "outputId": "a29cbaa3-0421-42e6-b60a-02ab7db5ee33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.1/803.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.7/205.7 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.1/305.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.0/70.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install -q -U langchain-google-genai langchain gradio openai"
      ],
      "id": "2d8859cb5d4d05fa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "urM6drkqCKrB"
      },
      "source": [
        "## Importando librerias"
      ],
      "id": "urM6drkqCKrB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3ba6357d830d135"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "import gradio as gr\n",
        "import time"
      ],
      "id": "a3ba6357d830d135"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb08191142e5242b"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import HumanMessage, SystemMessage"
      ],
      "id": "fb08191142e5242b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "OqE0_hi5CKrC"
      },
      "source": [
        "## Configurando keys"
      ],
      "id": "OqE0_hi5CKrC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab2c0037c1b9d28b",
        "outputId": "52697721-671f-4730-f54a-cd145e039d8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the secret value: ··········\n"
          ]
        }
      ],
      "source": [
        "GOOGLE_API_KEY = getpass('Enter the secret value: ')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
      ],
      "id": "ab2c0037c1b9d28b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e592d6ebd4ccb0e5",
        "outputId": "46ec5664-0c92-4fc2-84d7-2b569b344437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the secret value: ··········\n"
          ]
        }
      ],
      "source": [
        "OPENAI_API_KEY = getpass('Enter the secret value: ')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "id": "e592d6ebd4ccb0e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "qKmU_tHmCKrC"
      },
      "source": [
        "# Gemini vs GPT4  - Texto"
      ],
      "id": "qKmU_tHmCKrC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "d27cfa5934abb46e"
      },
      "source": [
        "## Google Gemini + Langchain"
      ],
      "id": "d27cfa5934abb46e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56c56c9cc2cb1da7"
      },
      "outputs": [],
      "source": [
        "llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-pro\",temperature=0.8, max_tokens=2000)\n",
        "response = llm_gemini.invoke(\"Quien eres?\")"
      ],
      "id": "56c56c9cc2cb1da7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3fe9ef83e2b62cfa",
        "outputId": "cb16d87c-9b05-4fd8-d46d-d71ab6472ec7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Soy un gran modelo de lenguaje, entrenado por Google.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "response.content"
      ],
      "id": "3fe9ef83e2b62cfa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "3247d1eaf3dea5ef"
      },
      "source": [
        "## OpenAI GPT-4 + Langchain"
      ],
      "id": "3247d1eaf3dea5ef"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93c180cd166233e9"
      },
      "outputs": [],
      "source": [
        "chat = ChatOpenAI( model=\"gpt-4\",temperature=0.8, max_tokens=2000,)"
      ],
      "id": "93c180cd166233e9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a388828c5f6c395e"
      },
      "outputs": [],
      "source": [
        "template = (\n",
        "    \"You are a helpful assistant\"\n",
        ")\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template = \"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
      ],
      "id": "a388828c5f6c395e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49e3b7ea999d5f3d"
      },
      "outputs": [],
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [system_message_prompt, human_message_prompt]\n",
        ")\n",
        "\n",
        "# get a chat completion from the formatted messages\n",
        "response = chat(\n",
        "    chat_prompt.format_prompt(\n",
        "        text=\"Quien eres?\"\n",
        "    ).to_messages()\n",
        ")"
      ],
      "id": "49e3b7ea999d5f3d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6d18e1cc1b228f45",
        "outputId": "33ca360c-614e-47aa-a945-86197cf8da06"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Soy un asistente inteligente de inteligencia artificial diseñado para ayudar con la información y realizar tareas.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "response.content"
      ],
      "id": "6d18e1cc1b228f45"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "cfd3915ed553c54c"
      },
      "source": [
        "## Funciones"
      ],
      "id": "cfd3915ed553c54c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebb96a1abe381809"
      },
      "outputs": [],
      "source": [
        "def gemini_response(text):\n",
        "    start_time = time.time()  # Inicia el contador de tiempo\n",
        "    response = llm_gemini.invoke(text) # Invoca el modelo\n",
        "    end_time = time.time()  # Finaliza el contador de tiempo\n",
        "    execution_time = end_time - start_time\n",
        "    return response.content, execution_time"
      ],
      "id": "ebb96a1abe381809"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "907943b6684ab689"
      },
      "outputs": [],
      "source": [
        "def gpt4_response(text):\n",
        "    start_time = time.time()  # Inicia el contador de tiempo\n",
        "\n",
        "    response = chat(\n",
        "        chat_prompt.format_prompt(\n",
        "            text=text\n",
        "        ).to_messages()\n",
        "    ) # Invoca el modelo\n",
        "    end_time = time.time()  # Finaliza el contador de tiempo\n",
        "    execution_time = end_time - start_time\n",
        "    return response.content, execution_time"
      ],
      "id": "907943b6684ab689"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f25786f6090c7945"
      },
      "outputs": [],
      "source": [
        "def gemini_vs_gpt4(text_question):\n",
        "    gemini_list = []\n",
        "    gpt_list = []\n",
        "    gemini_content, gemini_execution_time = gemini_response(text_question)\n",
        "    gpt4_content, gpt4_execution_time = gpt4_response(text_question)\n",
        "\n",
        "    gemini_time_formatted = f\"{gemini_execution_time:.2f} segundos\"\n",
        "    gpt4_time_formatted = f\"{gpt4_execution_time:.2f} segundos\"\n",
        "\n",
        "    gemini_list.append((text_question, gemini_content))\n",
        "    gpt_list.append((text_question, gpt4_content))\n",
        "    return gemini_list, gemini_time_formatted, gpt_list, gpt4_time_formatted"
      ],
      "id": "f25786f6090c7945"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "-yhMuc5gCKrE"
      },
      "source": [
        "## Gradio"
      ],
      "id": "-yhMuc5gCKrE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "805de4e7d1b35ed1",
        "outputId": "b8e906cb-4755-4d91-f81d-7c888acc1802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://299a7325d9574bcff0.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://299a7325d9574bcff0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 POST http://localhost:43131/v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n",
            "ERROR:tornado.access:500 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 691.81ms\n",
            "ERROR:tornado.access:500 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 584.29ms\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised InternalServerError: 500 POST http://localhost:43131/v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://299a7325d9574bcff0.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            txt_time_gemini = gr.Textbox(label=\"Tiempo de ejecución de Gemini Pro\", interactive=False)\n",
        "            chat_response_gemini = gr.Chatbot(label=\"Respuesta de Gemini\")\n",
        "        with gr.Column():\n",
        "            txt_time_gpt = gr.Textbox(label=\"Tiempo de ejecución de GPT-4\", interactive=False)\n",
        "            chat_response_gpt = gr.Chatbot(label=\"Respuesta de GPT-4\",)\n",
        "    with gr.Row():\n",
        "        txt_question = gr.Textbox(label=\"Pregunta...\")\n",
        "        btn_send = gr.Button(value=\"Enviar\")\n",
        "        btn_send.click(\n",
        "            gemini_vs_gpt4,\n",
        "            inputs=txt_question,\n",
        "            outputs=[chat_response_gemini, txt_time_gemini, chat_response_gpt, txt_time_gpt]\n",
        "        )\n",
        "\n",
        "demo.launch(debug=True)\n"
      ],
      "id": "805de4e7d1b35ed1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2eefccd323641a6",
        "outputId": "944ea98d-13a3-433a-b094-13068bb7a01f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "demo.close()"
      ],
      "id": "c2eefccd323641a6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "IJ4InzX2CKrF"
      },
      "source": [
        "# Gemini vs gpt4 - Vision"
      ],
      "id": "IJ4InzX2CKrF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "-p4UNtS9CKrF"
      },
      "source": [
        "## Gemini vision"
      ],
      "id": "-p4UNtS9CKrF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWA9F9pmCKrF"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "from openai import OpenAI\n",
        "import base64\n",
        "import io"
      ],
      "id": "FWA9F9pmCKrF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQUSwngXCKrF"
      },
      "outputs": [],
      "source": [
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "id": "zQUSwngXCKrF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvWtdvn_CKrF"
      },
      "outputs": [],
      "source": [
        "model_vision = genai.GenerativeModel('gemini-pro-vision')"
      ],
      "id": "rvWtdvn_CKrF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4Z5qfXSCKrG"
      },
      "outputs": [],
      "source": [
        "def gemini_vision(img, query):\n",
        "    gemini_list = []\n",
        "    start_time = time.time()  # Inicia el contador de tiempo\n",
        "\n",
        "    response = model_vision.generate_content([query, img]) ## Inferencia al modelo de vision\n",
        "    response.resolve()\n",
        "\n",
        "    end_time = time.time()  # Finaliza el contador de tiempo\n",
        "    execution_time = end_time - start_time\n",
        "    gemini_time_formatted = f\"{execution_time:.2f} segundos\"\n",
        "\n",
        "    gemini_list.append((query, response.text)) # Formato de tupla en lista\n",
        "\n",
        "    return gemini_list, gemini_time_formatted"
      ],
      "id": "s4Z5qfXSCKrG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "1Tf1zZqyCKrG"
      },
      "source": [
        "## GPT4 vision"
      ],
      "id": "1Tf1zZqyCKrG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paPPwWxkCKrG"
      },
      "outputs": [],
      "source": [
        "client = OpenAI()"
      ],
      "id": "paPPwWxkCKrG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohLkeyCGCKrG"
      },
      "outputs": [],
      "source": [
        "def gpt_vision(img, query):\n",
        "    gpt_list = []\n",
        "\n",
        "    buffer = io.BytesIO()\n",
        "    img.save(buffer, format='JPEG')  # You can change 'JPEG' to the appropriate format of your image\n",
        "    byte_data = buffer.getvalue()\n",
        "    # Now encode this bytes object with base64\n",
        "    encode_image = base64.b64encode(byte_data).decode('utf-8')\n",
        "\n",
        "    start_time = time.time()  # Inicia el contador de tiempo\n",
        "    response = client.chat.completions.create(\n",
        "    model=\"gpt-4-vision-preview\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": query},\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": f\"data:image/jpeg;base64,{encode_image}\"\n",
        "                    },\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=300,\n",
        ")\n",
        "    end_time = time.time()  # Finaliza el contador de tiempo\n",
        "    execution_time = end_time - start_time\n",
        "    gemini_time_formatted = f\"{execution_time:.2f} segundos\"\n",
        "    gpt_list.append((query, response.choices[0].message.content)) # Formato de tupla en lista\n",
        "\n",
        "    return gpt_list , gemini_time_formatted"
      ],
      "id": "ohLkeyCGCKrG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "VmKyDi5SCKrG"
      },
      "source": [
        "## Todo junto"
      ],
      "id": "VmKyDi5SCKrG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX21VloMCKrG"
      },
      "outputs": [],
      "source": [
        "def geminiv_vs_gpt4v(img, query):\n",
        "\n",
        "    gemini_content, gemini_execution_time = gemini_vision(img, query) # Ejecutando Gemini Vision\n",
        "    gpt4_content, gpt4_execution_time = gpt_vision(img, query)  # Ejecutando GPT 4 Vision\n",
        "\n",
        "    return gemini_content, gemini_execution_time, gpt4_content, gpt4_execution_time"
      ],
      "id": "QX21VloMCKrG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "PEmVW79ICKrG"
      },
      "source": [
        "## Gradio interface - Vision"
      ],
      "id": "PEmVW79ICKrG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "LO5pLs-jCKrG",
        "outputId": "8630c50c-1b22-4416-aa8b-ab9b8a8d7877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://446ac7bbbc9414ba5f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://446ac7bbbc9414ba5f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://446ac7bbbc9414ba5f.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "with gr.Blocks() as demo_vision:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            txt_time_gemini = gr.Textbox(label=\"Tiempo de ejecución de Gemini Pro\", interactive=False)\n",
        "            chat_response_gemini = gr.Chatbot(label=\"Respuesta de Gemini Pro\")\n",
        "        with gr.Column():\n",
        "            txt_time_gpt = gr.Textbox(label=\"Tiempo de ejecución de GPT4\", interactive=False)\n",
        "            chat_response_gpt = gr.Chatbot(label=\"Respuesta de GPT4\")\n",
        "    with gr.Row():\n",
        "        img_question = gr.Image(label=\"Pregunta...\",type=\"pil\")\n",
        "        txt_question = gr.Textbox(label=\"Pregunta...\")\n",
        "    with gr.Row():\n",
        "        btn_send = gr.Button(value=\"Enviar\")\n",
        "        btn_send.click(\n",
        "            geminiv_vs_gpt4v,\n",
        "            inputs=[img_question,txt_question],\n",
        "            outputs=[chat_response_gemini, txt_time_gemini,chat_response_gpt,txt_time_gpt]\n",
        "        )\n",
        "\n",
        "demo_vision.launch(debug=True)"
      ],
      "id": "LO5pLs-jCKrG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2OhLJZ7CKrH",
        "outputId": "938e945e-b113-41a8-e416-d7c54a606667",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "demo_vision.close()"
      ],
      "id": "Q2OhLJZ7CKrH"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}